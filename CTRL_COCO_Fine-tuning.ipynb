{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import CTRLTokenizer, CTRLLMHeadModel, Trainer, TrainingArguments, AdamW\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = np.load('./splitted_data/x_train.npy')\n",
    "#y_train = np.load('./splitted_data/y_train.npy')\n",
    "\n",
    "#x_test = np.load('./splitted_data/x_test.npy')\n",
    "#y_test = np.load('./splitted_data/y_test.npy')\n",
    "\n",
    "#x_val = np.load('./splitted_data/x_val.npy')\n",
    "#y_val = np.load('./splitted_data/y_val.npy')\n",
    "\n",
    "xy_train_validate = np.load('./splitted_data/xy_train_validate.npy')\n",
    "xy_test = np.load('./splitted_data/xy_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Train : {len(x_train)} ** Test : {len(x_test)} ** Val : {len(x_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A very clean and well decorated empty bathroom'\n",
      " 'A panoramic view of a kitchen and all of its appliances.'\n",
      " 'A blue and white bathroom with butterfly themed wall tiles.' ...\n",
      " 'Two women sit and pose with stuffed animals.'\n",
      " 'White Plate with a lot of guacamole and an extra large dollop of sour cream over meat'\n",
      " 'A dinner plate has a lemon wedge garnishment.'] \n",
      "\n",
      " 414113\n"
     ]
    }
   ],
   "source": [
    "print(xy_train_validate, \"\\n\\n\", len(xy_train_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_train_validate = xy_train_validate.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414113"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xy_train_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictPrint(dict):\n",
    "    for key, value in dict.items():\n",
    "        print(\"\\n{}:\\n\\n\\t{}\".format(key,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CTRLTokenizer.from_pretrained('ctrl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting special tokens\n",
    "tokenizer.add_special_tokens({'bos_token': '<BOS>', 'eos_token': '<EOS>','pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_length(sentences):\n",
    "    out = 0\n",
    "    for sentence in sentences:\n",
    "        if len(sentence.split()) > out:\n",
    "            out = len(sentence.split())\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max length for padding\n",
    "MAX_LENGTH = compute_max_length(xy_train_validate) + 1 #Â adding one to count the control code\n",
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCO_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, captions, tokenizer, control_code): # captions intere o spezzate?\n",
    "        self.captions = captions\n",
    "        self.tokenizer = tokenizer\n",
    "        self.control_code = control_code\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        outCaption = self.control_code + \" \" + self.captions[index]\n",
    "        outDict = self.tokenizer(outCaption, return_tensors = 'pt', padding='max_length', max_length=MAX_LENGTH, truncation = True)\n",
    "        \n",
    "        return {'input_ids': outDict['input_ids'],\n",
    "                'attention_mask': outDict['attention_mask'],\n",
    "                'token_type_ids': outDict['token_type_ids']\n",
    "               }\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = COCO_dataset(xy_train_validate[:300000], tokenizer, \"captions\")\n",
    "evaluate = COCO_dataset(xy_train_validate[300000:], tokenizer, \"captions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = COCO_dataset(xy_train_validate[:100], tokenizer, \"captions\")\n",
    "evaluate = COCO_dataset(xy_train_validate[100:150], tokenizer, \"captions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CTRLLMHeadModel(\n",
       "  (transformer): CTRLModel(\n",
       "    (w): Embedding(246534, 1280)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (12): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (13): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (14): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (15): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (16): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (17): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (18): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (19): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (20): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (21): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (22): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (23): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (24): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (25): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (26): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (27): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (28): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (29): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (30): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (31): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (32): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (33): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (34): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (35): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (36): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (37): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (38): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (39): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (40): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (41): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (42): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (43): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (44): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (45): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (46): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (47): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=246534, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CTRL model instance\n",
    "model = CTRLLMHeadModel.from_pretrained('ctrl')\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(246537, 1280)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ð¤ Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train,         # training dataset\n",
    "    eval_dataset=evaluate            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() # to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate() #Â to evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with a custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "accumulation_steps = 10\n",
    "evaluation_steps = 50\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_sampler = RandomSampler(train)\n",
    "train_dataloader = DataLoader(train, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_sampler = SequentialSampler(evaluate)\n",
    "validation_dataloader = DataLoader(evaluate, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,\n",
    "                  eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "total_steps = len(train_dataloader) * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT metric\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CTRLLMHeadModel(\n",
       "  (transformer): CTRLModel(\n",
       "    (w): Embedding(246537, 1280)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (12): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (13): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (14): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (15): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (16): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (17): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (18): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (19): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (20): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (21): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (22): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (23): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (24): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (25): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (26): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (27): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (28): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (29): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (30): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (31): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (32): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (33): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (34): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (35): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (36): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (37): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (38): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (39): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (40): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (41): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (42): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (43): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (44): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (45): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (46): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (47): EncoderLayer(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (Wq): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wk): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (Wv): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=8192, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=8192, out_features=1280, bias=True)\n",
       "        )\n",
       "        (layernorm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=246537, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=\"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "Batch 0 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 13.543726921081543\n",
      "Batch 1 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 12.848766326904297\n",
      "Batch 2 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 12.355555534362793\n",
      "Batch 3 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 11.701601028442383\n",
      "Batch 4 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 11.223841667175293\n",
      "Batch 5 loaded on device (cpu)\n",
      "  Batch     5  of     10.    Elapsed: 0:01:28.\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 10.928218841552734\n",
      "Batch 6 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 10.704354286193848\n",
      "Batch 7 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 10.131542205810547\n",
      "Batch 8 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 9.90682315826416\n",
      "Batch 9 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 9.699237823486328\n",
      "\n",
      "  Average training loss: 11.30\n",
      "  Training epcoh took: 0:02:53\n",
      "\n",
      "Running Validation...\n",
      "Batch 0 loaded on device (cpu)\n",
      "tensor(9.0478)\n",
      "Batch 1 loaded on device (cpu)\n",
      "tensor(8.9986)\n",
      "Batch 2 loaded on device (cpu)\n",
      "tensor(8.9173)\n",
      "Batch 3 loaded on device (cpu)\n",
      "tensor(9.0952)\n",
      "Batch 4 loaded on device (cpu)\n",
      "tensor(8.9717)\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "Batch 0 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 9.461292266845703\n",
      "Batch 1 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 8.902631759643555\n",
      "Batch 2 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 8.789836883544922\n",
      "Batch 3 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 8.588849067687988\n",
      "Batch 4 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 8.275691986083984\n",
      "Batch 5 loaded on device (cpu)\n",
      "  Batch     5  of     10.    Elapsed: 0:01:25.\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 8.145084381103516\n",
      "Batch 6 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 8.019412994384766\n",
      "Batch 7 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 7.871252059936523\n",
      "Batch 8 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 7.612861633300781\n",
      "Batch 9 loaded on device (cpu)\n",
      "Gradients put to zero.\n",
      "Output computed.\n",
      "Loss: 7.723505973815918\n",
      "\n",
      "  Average training loss: 8.34\n",
      "  Training epcoh took: 0:02:50\n",
      "\n",
      "Running Validation...\n",
      "Batch 0 loaded on device (cpu)\n",
      "tensor(6.9749)\n",
      "Batch 1 loaded on device (cpu)\n",
      "tensor(6.8594)\n",
      "Batch 2 loaded on device (cpu)\n",
      "tensor(6.7613)\n",
      "Batch 3 loaded on device (cpu)\n",
      "tensor(7.0152)\n",
      "Batch 4 loaded on device (cpu)\n",
      "tensor(6.9103)\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    \n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "    \n",
    "    model.zero_grad()\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        batch['input_ids'], batch['token_type_ids'], batch['attention_mask'] = batch['input_ids'].to(device), batch['token_type_ids'].to(device), batch['attention_mask'].to(device)\n",
    "        \n",
    "        #Â Logging\n",
    "        print(\"Batch {} loaded on device ({})\".format(step, device))\n",
    "        \n",
    "        # Progress update every 40 batches.\n",
    "        if step % 5 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        \n",
    "        \n",
    "        model.zero_grad()\n",
    "        print(\"Gradients put to zero.\")\n",
    "        \n",
    "        outputs = model(input_ids = batch['input_ids'], \n",
    "                        token_type_ids= batch['token_type_ids'] , \n",
    "                        attention_mask= batch['attention_mask'],\n",
    "                        labels = batch['input_ids'])\n",
    "        \n",
    "        print(\"Output computed.\")\n",
    "        \n",
    "        # The call to `model` may return a tuple or an object of CausalLMOutputWithPast class.\n",
    "        #Â In the first case, loss has to be extracted through index, while in the latter case,\n",
    "        # it's an attribute.\n",
    "        \n",
    "        if isinstance(outputs, CausalLMOutputWithPast):\n",
    "            loss = outputs.loss\n",
    "        else:\n",
    "            loss = outputs[0]\n",
    "        \n",
    "        \n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        print(\"Loss: \" + str(loss.item()))\n",
    "        total_loss = total_loss + loss.item()\n",
    "        \n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "        \n",
    "        \"\"\"\n",
    "        #### Accumulating gradient version ####\n",
    "        \n",
    "        #Average loss over accumulation steps\n",
    "        loss = loss / accumulation_steps\n",
    "        \n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "            optimizer.step()                            # Now we can do an optimizer step\n",
    "            scheduler.step()\n",
    "            model.zero_grad()                           # Reset gradients tensors\n",
    "        if (i+1) % evaluation_steps == 0:           # Evaluate the model when we...\n",
    "            evaluate_model()\n",
    "        \n",
    "        \n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "        \"\"\"\n",
    "    \n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation NOT SURE ABOUT VALIDATION BECAUSE \n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    # Evaluate data for one epoch\n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch['input_ids'], batch['token_type_ids'], batch['attention_mask'] = batch['input_ids'].to(device), batch['token_type_ids'].to(device), batch['attention_mask'].to(device)\n",
    "                \n",
    "        #Â Logging\n",
    "        print(\"Batch {} loaded on device ({})\".format(step, device))\n",
    "    \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(batch['input_ids'], \n",
    "                            token_type_ids=batch['token_type_ids'], \n",
    "                            attention_mask=batch['attention_mask'],\n",
    "                            labels = batch['input_ids'])\n",
    "        print(outputs.loss)\n",
    "        \n",
    "        #### CAPIRE COME VALUTARE LA VALIDATION\n",
    "        '''\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))'''\n",
    "    \n",
    "    \n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comandi utili per metrica BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/geek-ai/Texygen.git Texygen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Texygen.utils.metrics.Bleu import Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"The cat is on the table\", return_tensors=\"pt\")\n",
    "generation_output = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(token_ids = generation_output[0], skip_special_tokens = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"demofile2.txt\", \"w\")\n",
    "f.write(\"The cat is on the table and also the pen is on the table\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"demofile1.txt\", \"w\")\n",
    "f.write(\"The cat is on the table de la table de la table de la table de la table de la\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = Bleu(test_text='demofile1.txt', real_text='demofile2.txt', gram=3)\n",
    "bleu.get_score() #non funziona"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
