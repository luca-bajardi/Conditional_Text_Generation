{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING AND SHOWCASING \n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import copy\n",
    "reference = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']] #deve essere cosi \n",
    "candidate = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', 'from', 'space']\n",
    "chencherry = SmoothingFunction() #nel caso in cui non ci sono parole uguali, almeno non da 0 ma è less-harsh, pià utile per le comparison\n",
    "\n",
    "#Explanation of n-grams (difference between bleu-1 (not used) vs bleu-2 vs bleu-3 vs bleu-4 vs bleu-5)\n",
    "'''\n",
    "In practice, however, using individual words as the unit of comparison is not optimal. \n",
    "Instead, BLEU computes the same modified precision metric using n-grams. \n",
    "The length which has the \"highest correlation with monolingual human judgements\"[5] \n",
    "was found to be four. The unigram scores are found to account for the adequacy of the translation,\n",
    "how much information is retained. The longer n-gram scores account for the fluency of the translation,\n",
    "or to what extent it reads like \"good English\".\n",
    "https://en.wikipedia.org/wiki/BLEU\n",
    "'''\n",
    "\n",
    "#bleu-2\n",
    "weights = (1./2.,1./2.,1./2.)\n",
    "score = sentence_bleu(reference, candidate, weights,smoothing_function = chencherry.method1)\n",
    "\n",
    "#bleu-3\n",
    "weights = (1./3.,1./3.,1./3.)\n",
    "score = sentence_bleu(reference, candidate, weights,smoothing_function = chencherry.method1)\n",
    "\n",
    "#bleu-4 is standard, you could omit the weights\n",
    "weights = (1./4.,1./4.,1./4.,1./4.)\n",
    "score = sentence_bleu(reference, candidate, weights, smoothing_function = chencherry.method1)\n",
    "\n",
    "#bleu-5\n",
    "weights = (1./5., 1./5., 1./5., 1./5., 1./5.)\n",
    "score = sentence_bleu(reference, candidate, weights,smoothing_function = chencherry.method1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BLEU SCORE FUNCTION AND FUNCTIONALITY TEST ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(references, model_output, k):\n",
    "    '''\n",
    "    references is the y_true, list of string\n",
    "    model output, list of string\n",
    "    k is the number of n-grams you wanto to consider, from 2 to 5 is acceptable\n",
    "    \n",
    "    the \"right\" sentence for model_output[i] is references[i].\n",
    "    '''\n",
    "    if len(references) != len(model_output):\n",
    "        print(\"Error, references and model_output must be same length\")\n",
    "        return -1\n",
    "    if k < 2 or k > 5:\n",
    "        print(\"Error, k must be between 2 and 5\")\n",
    "        return -2\n",
    "    \n",
    "    N = len(references)\n",
    "    sum_scores = 0\n",
    "    chencherry = SmoothingFunction()\n",
    "    \n",
    "    if k == 2:\n",
    "        weights = (1./2.,1./2.)\n",
    "    elif k == 3:\n",
    "        weights = (1./3.,1./3.,1./3.)\n",
    "    elif k == 4:\n",
    "        weights = (1./4.,1./4.,1./4.,1./4.)\n",
    "    else:\n",
    "        #k==5\n",
    "        weights = (1./5.,1./5.,1./5.,1./5.,1./5.)\n",
    "    \n",
    "    \n",
    "    for i in range(N):\n",
    "        ref = references[i]\n",
    "        out = model_output[i]\n",
    "        \n",
    "        ref_words = str.split(ref)\n",
    "        out_words = str.split(out)\n",
    "        \n",
    "        score_i = sentence_bleu([ref_words],out_words,weights,smoothing_function = chencherry.method1)\n",
    "        \n",
    "        sum_scores += score_i\n",
    "        \n",
    "    return sum_scores/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING BLEU SCORE ## \n",
    "references = [\"The cat is on the table\", \"The red fox was looking at the sheep\"]\n",
    "model_output = [\"The cat is on the table\", \"The blue fox is looking at the moon\"]\n",
    "bleu_score(references, model_output, 2)\n",
    "bleu_score(references,model_output,3)\n",
    "bleu_score(references,model_output,4)\n",
    "bleu_score(references,model_output,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SELF-BLEU FUNCTION ### \n",
    "\n",
    "def self_bleu_score(model_output, k):\n",
    "    '''\n",
    "    model_output has list of strings\n",
    "    k is the n-grams number n as before\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    N = len(model_output)\n",
    "    \n",
    "    sum_scores = 0\n",
    "    \n",
    "    chencherry = SmoothingFunction()\n",
    "    \n",
    "    if k == 2:\n",
    "        weights = (1./2.,1./2.)\n",
    "    elif k == 3:\n",
    "        weights = (1./3.,1./3.,1./3.)\n",
    "    elif k == 4:\n",
    "        weights = (1./4.,1./4.,1./4.,1./4.)\n",
    "    else:\n",
    "        #k==5\n",
    "        weights = (1./5.,1./5.,1./5.,1./5.,1./5.)\n",
    "    \n",
    "    outputs = []\n",
    "    for el in model_output:\n",
    "        outputs.append(str.split(el))\n",
    "    \n",
    "    for i in range(N):\n",
    "        out_words_copied = outputs.copy()\n",
    "        #remove the i-esim string\n",
    "        candidate = out_words_copied.pop(i)\n",
    "        \n",
    "        score_i = sentence_bleu(out_words_copied,candidate,weights,smoothing_function = chencherry.method1)\n",
    "        \n",
    "        sum_scores += score_i\n",
    "    \n",
    "    \n",
    "    return sum_scores/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING SELF-BLEU SCORE ## \n",
    "references = [\"The cat is on the table\", \"The red fox was looking at the sheep\", \"The kid is talking\"]\n",
    "model_output = [\"The cat is on the table\", \"The blue fox is looking at the moon\", \"The kid was sleeping\"]\n",
    "self_bleu_score(model_output,2)\n",
    "self_bleu_score(model_output,3)\n",
    "self_bleu_score(model_output,4)\n",
    "self_bleu_score(model_output,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### POS-BLEU FUNCTION ###\n",
    "def pos_bleu_score(references, model_output, k):\n",
    "    '''\n",
    "    references is the y_true, list of string\n",
    "    model output, list of string\n",
    "    k is the number of n-grams you wanto to consider, from 2 to 5 is acceptable\n",
    "    \n",
    "    the \"right\" sentence for model_output[i] is references[i].\n",
    "    \n",
    "    This is the same as bleu_score, with the only difference that now the senteces \n",
    "    (both reference and model_outputs) are changed as Part of speech tags\n",
    "    '''\n",
    "    if len(references) != len(model_output):\n",
    "        print(\"Error, references and model_output must be same length\")\n",
    "        return -1\n",
    "    if k < 2 or k > 5:\n",
    "        print(\"Error, k must be between 2 and 5\")\n",
    "        return -2\n",
    "    \n",
    "    N = len(references)\n",
    "    sum_scores = 0\n",
    "    chencherry = SmoothingFunction()\n",
    "    \n",
    "    if k == 2:\n",
    "        weights = (1./2.,1./2.)\n",
    "    elif k == 3:\n",
    "        weights = (1./3.,1./3.,1./3.)\n",
    "    elif k == 4:\n",
    "        weights = (1./4.,1./4.,1./4.,1./4.)\n",
    "    else:\n",
    "        #k==5\n",
    "        weights = (1./5.,1./5.,1./5.,1./5.,1./5.)\n",
    "    \n",
    "    \n",
    "    for i in range(N):\n",
    "        ref = references[i]\n",
    "        out = model_output[i]\n",
    "        ref_words = word_tokenize(ref)\n",
    "        out_words = word_tokenize(out)\n",
    "        \n",
    "        ref_pos = pos_tag(ref_words)\n",
    "        out_pos = pos_tag(out_words)\n",
    "        \n",
    "        ref_words = []\n",
    "        for i in range(len(ref_pos)):\n",
    "            ref_words.append(ref_pos[i][1])\n",
    "        \n",
    "        out_words = []\n",
    "        for i in range(len(out_pos)):\n",
    "            out_words.append(out_pos[i][1])\n",
    "        \n",
    "        score_i = sentence_bleu([ref_words],out_words,weights,smoothing_function = chencherry.method1)\n",
    "        \n",
    "        sum_scores += score_i\n",
    "        \n",
    "    return sum_scores/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING POS-BLEU SCORE ## \n",
    "references = [\"The cat is on the table\", \"The red fox was looking at the sheep\"]\n",
    "model_output = [\"The cat is on the table\", \"The blue fox is looking at the moon\"]\n",
    "pos_bleu_score(references, model_output,2)\n",
    "pos_bleu_score(references,model_output,3)\n",
    "pos_bleu_score(references,model_output,4)\n",
    "pos_bleu_score(references,model_output,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python_3_7",
   "language": "python",
   "name": "python_3_7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
